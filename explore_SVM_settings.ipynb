{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Settings of SVM\n",
    "\n",
    "*Yuhui Hong <yuhhong@iu.edu>*\n",
    "\n",
    "Here, the 'Hillary Clinton' is used as an example aiming to explore the affect of each parameter of SVM. Then we optimized them for different targets one by one, the final results are in `a.py`. \n",
    "\n",
    "## 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from a_util import TweetsData\n",
    "from a import per_SVM\n",
    "\n",
    "# change paths if necessary\n",
    "TRAIN_SET_PATH = './StanceDataset/train.csv'\n",
    "TEST_SET_PATH = './StanceDataset/test.csv'\n",
    "TARGET_LIST = ['Hillary Clinton', 'Climate Change is a Real Concern', 'Legalization of Abortion', 'Atheism', 'Feminist Movement']\n",
    "STANCE_DICT = {'AGAINST': 0, 'NONE': 1, 'FAVOR': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2914 training data from ./StanceDataset/train.csv\n",
      "Load 1956 test data from ./StanceDataset/test.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 1: Read in train.csv and test.csv. \n",
    "# 'latin1' resolves UniCode decode error\n",
    "df_train = pd.read_csv(TRAIN_SET_PATH, engine='python', dtype='str', encoding ='latin1') \n",
    "df_test = pd.read_csv(TEST_SET_PATH, engine='python', dtype='str', encoding ='latin1')\n",
    "\n",
    "### 2: Preprocess on data (details in `a_1_2_util.py`).\n",
    "### 3: Extract a bag-of-words list of nouns, adj, and verbs from original Tweets.\n",
    "data_train = TweetsData(df_train) # init a TweetsData\n",
    "print(\"Load {} training data from {}\".format(len(data_train), TRAIN_SET_PATH))\n",
    "data_test = TweetsData(df_test) # init a TweetsData\n",
    "print(\"Load {} test data from {}\\n\".format(len(data_test), TEST_SET_PATH))\n",
    "# print(\"Targets in train: {}\".format(data_train.get_targets())) \n",
    "# print(\"Targets in test: {}\".format(data_test.get_targets()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6305084745762712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=1.0, kernel='rbf', decision_function_shape='ovr', class_weight=None)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel type\n",
    "\n",
    "**kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’**\n",
    "\n",
    "Related parameters: \n",
    "\n",
    "- **degree: int, default=3**\n",
    "- **gamma: {‘scale’, ‘auto’} or float, default=’scale’**\n",
    "- **coef0: float, default=0.0**\n",
    "\n",
    "As experience, the linear and rbf kernels are most commonly used kernels. If the dimension of feature is large enough, the data can be linearly seperatable in high dimensionality, the linear kernel will performance great and fast. If the dimension of feature is not large enough, rbf kernel could be a good choice. Then we will run them one by one and sdjust the related parameters. \n",
    "\n",
    "### 2.1 kernel = 'linear'\n",
    "\n",
    "$$K(x_i,x_j)=x_i^Tx_j$$\n",
    "\n",
    "As the above equation shows, the linear kernel does not need other related parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6101694915254238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 kernel = 'poly'\n",
    "\n",
    "$$K(x_i,x_j)=(\\gamma x_i^Tx_j + r)^d, d>1$$\n",
    "\n",
    "As the above equation shows, the polynomial kernel need 3 parameters, $d$, $\\gamma$, $r$. We need to ajust all the related parameters one by one. \n",
    "\n",
    "Here is the default polynomial kernel settings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.5830508474576271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=3, gamma='scale', coef0=0)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adjust $d$! The best degree is $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6305084745762712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=1)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adjust $\\gamma$! There are three choices, 'auto', 'scale' (default) and other float number. \n",
    "\n",
    "- If `gamma='scale'` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma.\n",
    "\n",
    "- If `gamma='auto'`, uses 1 / n_features.\n",
    "\n",
    "The larger the gamma, the fewer support vectors, and the smaller the gamma value, the more support vectors. More support vectors could fit the model better for the training data, however, may lead to a bad performance on test data. Here, we only try 'scale' and 'auto' of $\\gamma$ rather than other float numbers, and 'scale' performance better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6305084745762712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=1, gamma='scale')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adjust $r$! When degree equals to $1$, $r$ does not affect to the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6305084745762712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=1, gamma='scale', coef0=100)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 kernel = 'rbf'\n",
    "\n",
    "$$K(x_i,x_j)=exp(-\\gamma ||x_i-x_j||^2),\\gamma>0$$\n",
    "\n",
    "As the above equation shows, the radial basis function (RBF) kernel need 1 parameters, $\\gamma$. \n",
    "\n",
    "Similar to the experiment above, we adjusted $\\gamma$. The results show that 'scale' performance better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6305084745762712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', gamma='scale')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 kernel = 'sigmoid'\n",
    "\n",
    "$$K(x_i,x_j)=tanh(\\gamma x_i^Tx_j + r ), \\gamma>0, r<0$$\n",
    "\n",
    "As the above equation shows, the sigmoid kernel need 2 parameters, $\\gamma$, $r$. \n",
    "\n",
    "Similar to the experiment above, we adjusted $\\gamma$ and $r$. The results show that `gamma='scale'` and `coef0=0` performances better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6237288135593221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='sigmoid', gamma='scale', coef0=0)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularization parameter\n",
    "\n",
    "**C: float, default=1.0**\n",
    "\n",
    "Refer: [Intuition for the regularization parameter in SVM](https://datascience.stackexchange.com/questions/4943/intuition-for-the-regularization-parameter-in-svm)\n",
    "\n",
    "The regularization parameter (lambda) serves as a degree of importance that is given to misclassifications. SVM poses a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the number of misclassifications. However, for non-separable problems, in order to find a solution, the misclassification constraint must be relaxed, and this is done by setting the mentioned \"regularization\". \n",
    "\n",
    "If the regularization parameter is too small, the model will be underfitted. If the regularization parameter is too large, the model will be overfitted. \n",
    "\n",
    "In this example, the best `C=10`. \n",
    "\n",
    "### 3.1 C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.5830508474576271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=0.1, kernel='rbf', gamma='scale')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6338983050847458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=10, kernel='rbf', gamma='scale')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6338983050847458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=100, kernel='rbf', gamma='scale')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class weight of regularization parameter\n",
    "\n",
    "**class_weight: dict or ‘balanced’, default=None**\n",
    "\n",
    "This parameter could set the regularization parameter `C` of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Let's try ‘balanced’ mode. \n",
    "\n",
    "In this example, the weight of classes does not effect the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6338983050847458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=10, kernel='rbf', gamma='scale', class_weight='balanced')\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best settings\n",
    "\n",
    "From all the experiments above, we could get a general optimization steps of SVM. \n",
    "\n",
    "- Check that which kernel performance best with their default related parameters. \n",
    "    In most of the situations, it is linear kernel or RBF kernel. If the dimension of feature is large enough compare to the number of samples, the data can be linearly seperatable in high dimensionality, the linear kernel will performance great and fast. If the dimension of feature is not large enough, rbf kernel could be a good choice. \n",
    "- Adjust the regularization parameter.\n",
    "- Check that whether a balanced class weight need to be used.\n",
    "\n",
    "Then we know the best settings of this example are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hillary Clinton\n",
      "X_train: (689, 3127), Y_train: (689,)\n",
      "X_test: (295, 3127), Y_test: (295,)\n",
      "Training the SVM...\n",
      "Done!\n",
      "Accuracy score: 0.6338983050847458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=10, kernel='rbf', gamma='scale', class_weight=None)\n",
    "per_SVM(data_train, data_test, clf, target='Hillary Clinton')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36ae728f10cb4a9fb53c3fd03aaea402af50bd5d5f009e385e6fbcc34fa73efc"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
